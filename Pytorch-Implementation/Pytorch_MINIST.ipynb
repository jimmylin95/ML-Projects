{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'mnist'\n",
    "batch_size = 64\n",
    "epochs = 10\n",
    "lr = 0.01\n",
    "\n",
    "if dataset == 'mnist':\n",
    "    data_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,))\n",
    "    ])\n",
    "    trainset = datasets.MNIST(root='.', train=True, download=True, transform=data_transform)\n",
    "    testset = datasets.MNIST(root='.', train=False, download=True, transform=data_transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "test_loader  = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 1, 28, 28])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images, labels = next(iter(train_loader))\n",
    "images.shape\n",
    "# we use view to reshape the data to an appropriate input for the model, we know we have a batch size of 64\n",
    "# but we don't know anything else, so we flatten the rest into 28 x 28 x 1 = 784"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = images.view(64,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=784, out_features=128, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=64, out_features=10, bias=True)\n",
      "  (5): LogSoftmax()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Now that we have defined the model, we need to test run it once to see if it works\n",
    "input_size = 784\n",
    "hidden_sizes = [128, 64]\n",
    "output_size = 10\n",
    "\n",
    "# Build a feed-forward network\n",
    "model = nn.Sequential(nn.Linear(input_size, hidden_sizes[0]),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(hidden_sizes[1], output_size),\n",
    "                      nn.LogSoftmax(dim = 1))\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test run 1\n",
    "predicted_y = model(images)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "loss = loss_func(predicted_y, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before backward pass: \n",
      " None\n",
      "After backward pass: \n",
      " tensor([[-0.0036, -0.0036, -0.0036,  ..., -0.0036, -0.0036, -0.0036],\n",
      "        [-0.0005, -0.0005, -0.0005,  ..., -0.0005, -0.0005, -0.0005],\n",
      "        [-0.0005, -0.0005, -0.0005,  ..., -0.0005, -0.0005, -0.0005],\n",
      "        ...,\n",
      "        [ 0.0027,  0.0027,  0.0027,  ...,  0.0027,  0.0027,  0.0027],\n",
      "        [ 0.0044,  0.0044,  0.0044,  ...,  0.0044,  0.0044,  0.0044],\n",
      "        [ 0.0033,  0.0033,  0.0033,  ...,  0.0033,  0.0033,  0.0033]])\n"
     ]
    }
   ],
   "source": [
    "# Up to this part this morning\n",
    "\n",
    "# What are the gradients \n",
    "print('Before backward pass: \\n', model[0].weight.grad)\n",
    "loss.backward()\n",
    "print('After backward pass: \\n', model[0].weight.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - Training loss: 5451.524821356098\n",
      "Epoch 1 - Training loss: 2.379009628346734\n",
      "Epoch 2 - Training loss: 2.377739315348139\n",
      "Epoch 3 - Training loss: 2.3781830232534835\n",
      "Epoch 4 - Training loss: 2.3774945209783787\n",
      "Epoch 5 - Training loss: 2.382479196418323\n",
      "Epoch 6 - Training loss: 2.3773455121623934\n",
      "Epoch 7 - Training loss: 2.3840213221019266\n",
      "Epoch 8 - Training loss: 2.3793262812628675\n",
      "Epoch 9 - Training loss: 2.3806432137357145\n"
     ]
    }
   ],
   "source": [
    "# okay so we have one back prop\n",
    "# define optimizer SGD\n",
    "optimizer = optim.SGD(model.parameters(), lr=10)\n",
    "optimizer.zero_grad()\n",
    "epochs = 10\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    for images, labels in train_loader:\n",
    "        # Flatten MNIST images into a 784 long vector\n",
    "        images = images.view(images.shape[0], -1)\n",
    "    \n",
    "        # Training pass\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        predicted_y = model(images)\n",
    "        loss = loss_func(predicted_y, labels)\n",
    "        \n",
    "        #This is where the model learns by backpropagating\n",
    "        loss.backward()\n",
    "        \n",
    "        #And optimizes its weights here\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    print(\"Epoch {} - Training loss: {}\".format(e, running_loss/len(train_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# when we set the learning rate to 10, the training loss drops quickly form 5451.52 to 2, but stays constantly\n",
    "# around 2.37 and might sometimes increase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIFAR10 Data and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'mnist'\n",
    "batch_size = 64\n",
    "epochs = 10\n",
    "lr = 0.01\n",
    "\n",
    "if dataset == 'mnist':\n",
    "    data_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,))\n",
    "    ])\n",
    "    trainset = datasets.MNIST(root='.', train=True, download=True, transform=data_transform)\n",
    "    testset = datasets.MNIST(root='.', train=False, download=True, transform=data_transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "test_loader  = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
